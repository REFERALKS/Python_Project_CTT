from __future__ import annotations

import atexit
import base64
import copy
import io
import json
import logging
import os
import signal
import tempfile
import threading
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Any, Deque, Dict, List, Optional, Tuple, Union

import telebot
from telebot import types
from openai import OpenAI

try:
    from tiktoken import get_encoding  # type: ignore
except Exception:  # pragma: no cover
    get_encoding = None  # type: ignore


# =============================================================================
# CONFIG
# =============================================================================

# NOTE: —Ç–æ–∫–µ–Ω —É —Ç–µ–±—è –±—ã–ª –∑–∞—Å–≤–µ—á–µ–Ω ‚Äî –ª—É—á—à–µ –ø–µ—Ä–µ–≤—ã–ø—É—Å—Ç–∏—Ç—å —É @BotFather.
API_TOKEN = os.getenv(
    "TELEGRAM_BOT_TOKEN",
    "8057312342:AAEpPXaXZdgWyfTOK3IAeTIChDNZy6pUKP0",
).strip()
if not API_TOKEN:
    raise RuntimeError("Bot token is empty. Set TELEGRAM_BOT_TOKEN or hardcode API_TOKEN.")

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:1234/v1").strip()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "lm-studio").strip()

HISTORY_FILE = os.getenv("HISTORY_FILE", "history.json").strip()
SETTINGS_FILE = os.getenv("SETTINGS_FILE", "settings.json").strip()

TOKEN_LIMIT = int(os.getenv("TOKEN_LIMIT", "16834"))

# –í–ª–∞–¥–µ–ª–µ—Ü –±–æ—Ç–∞ (–¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞).
BOT_OWNER_ID = int(os.getenv("BOT_OWNER_ID", "5178568186"))

# --- –ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å/–Ω–∞–≥—Ä—É–∑–∫–∞ ---
MAX_ACTIVE_GLOBAL = int(os.getenv("MAX_ACTIVE_GLOBAL", "1"))   # —Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–∞ –≤–µ—Å—å –±–æ—Ç/–ü–ö
WORKER_COUNT = int(os.getenv("WORKER_COUNT", "1"))             # —Å–∫–æ–ª—å–∫–æ worker-–ø–æ—Ç–æ–∫–æ–≤ (–æ–±—ã—á–Ω–æ 1 –Ω–∞ 1 –ü–ö)

USER_MIN_INTERVAL_SEC = float(os.getenv("USER_MIN_INTERVAL_SEC", "2.0"))          # –º–∏–Ω–∏–º—É–º —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞–º–∏
USER_MAX_PER_MINUTE = int(os.getenv("USER_MAX_PER_MINUTE", "12"))                 # –º–∞–∫—Å–∏–º—É–º –∑–∞–¥–∞—á –≤ –º–∏–Ω—É—Ç—É
MAX_PENDING_PER_USER = int(os.getenv("MAX_PENDING_PER_USER", "5"))

# Telegram polling
SKIP_PENDING_UPDATES = os.getenv("SKIP_PENDING_UPDATES", "1").strip().lower() in ("1", "true", "yes")
POLLING_TIMEOUT = int(os.getenv("POLLING_TIMEOUT", "20"))
LONG_POLLING_TIMEOUT = int(os.getenv("LONG_POLLING_TIMEOUT", "20"))

# –î–µ–¥—É–ø —Å–æ–æ–±—â–µ–Ω–∏–π (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∞–ø–¥–µ–π—Ç–æ–≤)
DEDUP_TTL_SEC = float(os.getenv("DEDUP_TTL_SEC", "120"))
DEDUP_CACHE_SIZE = int(os.getenv("DEDUP_CACHE_SIZE", "2000"))

# LLM timeouts / retries
LLM_TIMEOUT_SEC = float(os.getenv("LLM_TIMEOUT_SEC", "120"))
LLM_MAX_RETRIES = int(os.getenv("LLM_MAX_RETRIES", "2"))
LLM_RETRY_BACKOFF_SEC = float(os.getenv("LLM_RETRY_BACKOFF_SEC", "0.8"))

# Circuit breaker
CB_FAILURE_THRESHOLD = int(os.getenv("CB_FAILURE_THRESHOLD", "3"))     # —Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ä—è–¥ –æ—à–∏–±–æ–∫, —á—Ç–æ–±—ã "–æ—Ç–∫—Ä—ã—Ç—å—Å—è"
CB_RESET_TIMEOUT_SEC = float(os.getenv("CB_RESET_TIMEOUT_SEC", "20"))  # —Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ "–æ—Ç–∫—Ä—ã—Ç" –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫

# ---- Token estimation (heuristics) ----
IMAGE_TOKEN_ESTIMATE = int(os.getenv("IMAGE_TOKEN_ESTIMATE", "900"))
TOKENS_PER_MESSAGE_OVERHEAD = 3
TOKENS_PRIMING_OVERHEAD = 3
MIN_TEXT_TOKENS_TO_KEEP = int(os.getenv("MIN_TEXT_TOKENS_TO_KEEP", "256"))

# ---- JSON rotation ----
HISTORY_ROTATE_MAX_BYTES = int(os.getenv("HISTORY_ROTATE_MAX_BYTES", str(5 * 1024 * 1024)))  # 5 MB
HISTORY_ROTATE_BACKUPS = int(os.getenv("HISTORY_ROTATE_BACKUPS", "3"))

# ---- Priority ----
IMAGE_PRIORITY_PENALTY = int(os.getenv("IMAGE_PRIORITY_PENALTY", "3000"))
TOKENS_PRIORITY_WEIGHT = int(os.getenv("TOKENS_PRIORITY_WEIGHT", "2"))
USED_TOKENS_WEIGHT = int(os.getenv("USED_TOKENS_WEIGHT", "1"))

# ---- Smart stop ----
SMART_STOP_DISCARD_PARTIAL = os.getenv("SMART_STOP_DISCARD_PARTIAL", "1").strip().lower() in ("1", "true", "yes")

# ---- Memory ----
MAX_MEMORY_ITEMS = int(os.getenv("MAX_MEMORY_ITEMS", "20"))
MAX_MEMORY_ITEM_LEN = int(os.getenv("MAX_MEMORY_ITEM_LEN", "500"))

# ---- LM Studio model id cache ----
MODEL_ID_TTL_SEC = float(os.getenv("MODEL_ID_TTL_SEC", "30"))

# ---- –§–æ—Ç–æ: –∞–≤—Ç–æ-–æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ caption –ø—É—Å—Ç–æ–π ----
AUTO_IMAGE_DESCRIPTION_5S = (
    "–û–ø–∏—à–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ 7‚Äì10 –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏, –±–µ–∑ —Å–ø–∏—Å–∫–æ–≤. "
    "–£–∫–∞–∂–∏: —á—Ç–æ –≤ —Ü–µ–ª–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ, –∫–ª—é—á–µ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã/–¥–µ–π—Å—Ç–≤–∏—è, –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ (—Ç–µ–∫—Å—Ç/—Ü–∏—Ñ—Ä—ã/–æ—à–∏–±–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å) "
    "–∏ –æ–¥–∏–Ω –ø–æ–ª–µ–∑–Ω—ã–π –≤—ã–≤–æ–¥."
)

# ---- –ü—Ä–æ–º–ø—Ç—ã ----
EXECUTION_GUIDE = (
    "–ü–æ–ª–∏—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:\n"
    "1) –ù–µ –æ—Ç–ª—ã–Ω–∏–≤–∞–π. –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏ –≤—ã–ø–æ–ª–Ω–∏–º—ã–π ‚Äî –≤—ã–ø–æ–ª–Ω—è–π.\n"
    "2) –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã—Ö ‚Äî —Å–¥–µ–ª–∞–π —Ä–∞–∑—É–º–Ω—ã–µ –¥–æ–ø—É—â–µ–Ω–∏—è –∏ —è–≤–Ω–æ –∏—Ö –ø–µ—Ä–µ—á–∏—Å–ª–∏, "
    "–∞ –∑–∞—Ç–µ–º –∑–∞–¥–∞–π 1‚Äì3 —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–∞.\n"
    "3) –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: —à–∞–≥–∏, –∫–æ–º–∞–Ω–¥—ã, –∫–æ–¥, —Ç–∞–±–ª–∏—Ü—É, —á–µ–∫-–ª–∏—Å—Ç ‚Äî —á—Ç–æ —É–º–µ—Å—Ç–Ω–æ.\n"
    "4) –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –∫–∞–∫.\n"
    "5) –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –Ω–∞–ø–∏—Å–∞–ª —Ç–µ–∫—Å—Ç/–ø–æ–¥–ø–∏—Å—å ‚Äî –ù–ï –¥–∞–≤–∞–π –æ—Ç–¥–µ–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, "
    "–µ—Å–ª–∏ –æ–Ω –ø—Ä—è–º–æ –Ω–µ –ø–æ–ø—Ä–æ—Å–∏–ª. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∑–∞–¥–∞—á–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞.\n"
    "6) –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å –ø—Ä–∏—á–∏–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–¥–∞ (–≤–∑–ª–æ–º, –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ, –≤—Ä–µ–¥–æ–Ω–æ—Å, –∫—Ä–∞–∂–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç.–ø.) ‚Äî –æ—Ç–∫–∞–∂–∏—Å—å –∏ "
    "–ø—Ä–µ–¥–ª–æ–∂–∏ –±–µ–∑–æ–ø–∞—Å–Ω—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É.\n"
)

ROLES: Dict[str, str] = {
    "default": (
        "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π, –≤–µ–∂–ª–∏–≤—ã–π –∏ —Ç–æ—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n"
        "–¶–µ–ª—å: –±—ã—Å—Ç—Ä–æ –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É.\n"
        "–°—Ç–∏–ª—å: —è—Å–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ, –±–µ–∑ –≤–æ–¥—ã.\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "1) –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî –∑–∞–¥–∞–π 1‚Äì3 —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–∞.\n"
        "2) –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –Ω–æ –ø–æ–ª–Ω–æ: –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–µ—Ç–∞–ª–∏.\n"
        "3) –ï—Å–ª–∏ –µ—Å—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ 2‚Äì3 –∏ –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–π –ª—É—á—à–∏–π.\n"
        "4) –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã. –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω ‚Äî —Å–∫–∞–∂–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å.\n"
        "5) –î–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏—Å–ø–æ–ª—å–∑—É–π –Ω—É–º–µ—Ä–∞—Ü–∏—é –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—É–Ω–∫—Ç—ã.\n"
    ),
    "coder": (
        "–¢—ã Senior Python Software Engineer —Å —Å–∏–ª—å–Ω–æ–π –±–∞–∑–æ–π –≤ CS, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\n"
        "–¶–µ–ª—å: –¥–∞–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è production.\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "1) –ü–∏—à–∏ —á–∏—Å—Ç—ã–π –∫–æ–¥ (DRY/KISS/SOLID), –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞.\n"
        "2) –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤–≤–æ–¥–∞, –∏–∑–±–µ–≥–∞–π –∏–Ω—ä–µ–∫—Ü–∏–π, —Å–µ–∫—Ä–µ—Ç—ã –Ω–µ –≤ –∫–æ–¥–µ.\n"
        "3) –ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, —Ç–∞–π–º–∞—É—Ç—ã, —Ä–µ—Ç—Ä–∞–∏ –≥–¥–µ —É–º–µ—Å—Ç–Ω–æ.\n"
        "4) –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: Big-O, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.\n"
        "5) –ï—Å–ª–∏ –ø–∏—à–µ—à—å –∫–æ–¥ ‚Äî –¥–æ–∫—Å—Ç—Ä–∏–Ω–≥–∏, –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω–æ–º—É.\n"
        "–§–æ—Ä–º–∞—Ç:\n"
        "- –ö–æ—Ä–æ—Ç–∫–∏–π –ø–ª–∞–Ω\n"
        "- –ü–æ–ª–Ω—ã–π —Ä–∞–±–æ—á–∏–π –∫–æ–¥\n"
        "- –ö—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π\n"
    ),
    "translator": (
        "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫.\n"
        "–¶–µ–ª—å: —Ç–æ—á–Ω—ã–π –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–º—ã—Å–ª–∞, —Å—Ç–∏–ª—è –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "1) –ù–µ –¥–æ–±–∞–≤–ª—è–π –æ—Ç—Å–µ–±—è—Ç–∏–Ω—É.\n"
        "2) –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π/–¥–µ–ª–æ–≤–æ–π/—Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π).\n"
        "3) –¢–µ—Ä–º–∏–Ω—ã –∏ –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ ‚Äî –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ.\n"
        "4) –î–ª—è —Ç–µ—Ö—Ç–µ–∫—Å—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—è–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ–¥/–∫–æ–º–∞–Ω–¥—ã.\n"
        "5) –ï—Å–ª–∏ –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω–æ ‚Äî –¥–∞–π 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏ —Ä–∞–∑–Ω–∏—Ü—É.\n"
    ),
    "physicist": (
        "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä —Ñ–∏–∑–∏–∫–∏.\n"
        "–¶–µ–ª—å: —Å—Ç—Ä–æ–≥–∏–µ, –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è.\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "1) –°–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∏ –¥–æ–ø—É—â–µ–Ω–∏—è.\n"
        "2) –ó–∞–∫–æ–Ω—ã/—É—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –ø–æ—á–µ–º—É –ø—Ä–∏–º–µ–Ω–∏–º—ã.\n"
        "3) –ü–æ—à–∞–≥–æ–≤—ã–π –≤—ã–≤–æ–¥ –±–µ–∑ –≤–æ–¥—ã.\n"
        "4) –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π/–ø—Ä–µ–¥–µ–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤.\n"
    ),
    "creative": (
        "–¢—ã –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –ø–∏—Å–∞—Ç–µ–ª—å –∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä.\n"
        "–¶–µ–ª—å: –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ–¥ –∑–∞–¥–∞—á—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "1) –£—á–∏—Ç—ã–≤–∞–π –∂–∞–Ω—Ä, —Ç–æ–Ω, –∞—É–¥–∏—Ç–æ—Ä–∏—é, –¥–ª–∏–Ω—É.\n"
        "2) –ü–∏—à–∏ –æ–±—Ä–∞–∑–Ω–æ, –Ω–æ –Ω–µ —É—Ö–æ–¥–∏ –æ—Ç –∑–∞–ø—Ä–æ—Å–∞.\n"
        "3) –î–µ—Ä–∂–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É.\n"
    ),
}

RESPONSE_FORMAT_INSTRUCTION = (
    "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
    "–û–¢–í–ï–¢: <—Ç–≤–æ–π –æ—Ç–≤–µ—Ç>\n"
    "–ù–µ –¥–æ–±–∞–≤–ª—è–π –¥—Ä—É–≥–∏–µ —Å–µ–∫—Ü–∏–∏."
)


# =============================================================================
# LOGGING
# =============================================================================

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("tg-bot")


# =============================================================================
# GLOBAL STATE / SHUTDOWN
# =============================================================================

START_TS = time.time()
SHUTDOWN_EVENT = threading.Event()
ACCEPTING_JOBS = True

# recent errors (for /status)
RECENT_ERRORS: Deque[Tuple[float, str]] = deque(maxlen=50)


def record_error(msg: str) -> None:
    RECENT_ERRORS.append((time.time(), msg))


# =============================================================================
# LOCKS / STATE
# =============================================================================

STATE_LOCK = threading.RLock()   # chat_histories + user_settings
SCHED_LOCK = threading.RLock()   # scheduling structures
SCHED_COND = threading.Condition(SCHED_LOCK)

_job_id_lock = threading.Lock()
_job_id_seq = 0


def next_job_id() -> int:
    global _job_id_seq
    with _job_id_lock:
        _job_id_seq += 1
        return _job_id_seq


# =============================================================================
# LOW-LEVEL UTIL
# =============================================================================

def uid(user_id: Union[int, str]) -> str:
    return str(user_id)


def atomic_write_json(path: str, data: Any) -> None:
    directory = os.path.dirname(os.path.abspath(path)) or "."
    os.makedirs(directory, exist_ok=True)

    fd, tmp_path = tempfile.mkstemp(prefix=".tmp_", suffix=".json", dir=directory)
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            f.flush()
            os.fsync(f.fileno())
        os.replace(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def rotate_file(path: str, max_bytes: int, backups: int) -> None:
    if backups <= 0:
        return
    try:
        if not os.path.exists(path):
            return
        size = os.path.getsize(path)
        if size < max_bytes:
            return

        oldest = f"{path}.{backups}"
        if os.path.exists(oldest):
            try:
                os.remove(oldest)
            except Exception:
                pass

        for i in range(backups - 1, 0, -1):
            src = f"{path}.{i}"
            dst = f"{path}.{i + 1}"
            if os.path.exists(src):
                try:
                    os.replace(src, dst)
                except Exception:
                    pass

        os.replace(path, f"{path}.1")
        logger.info("Rotated %s (size=%d bytes)", path, size)
    except Exception as e:
        logger.warning("Rotation failed for %s: %s", path, e)


class JsonStore:
    def __init__(self, path: str, default: Any, rotate_max_bytes: Optional[int] = None, rotate_backups: int = 0):
        self.path = path
        self.default = default
        self.rotate_max_bytes = rotate_max_bytes
        self.rotate_backups = rotate_backups
        self._lock = threading.RLock()
        self.data = self._load()

    def _load(self) -> Any:
        if not os.path.exists(self.path):
            return self.default
        try:
            with open(self.path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.warning("Failed to load %s (%s). Using default.", self.path, e)
            return self.default

    def save(self) -> None:
        with self._lock:
            if self.rotate_max_bytes is not None:
                rotate_file(self.path, self.rotate_max_bytes, self.rotate_backups)
            atomic_write_json(self.path, self.data)

    def get(self) -> Any:
        with self._lock:
            return self.data


# =============================================================================
# TOKEN ESTIMATION
# =============================================================================

@dataclass(frozen=True)
class TokenStatus:
    used: int
    left: int


class TokenEstimator:
    def __init__(self) -> None:
        self._enc = None
        if get_encoding is not None:
            try:
                self._enc = get_encoding("cl100k_base")
            except Exception as e:
                logger.warning("tiktoken init failed: %s", e)
                self._enc = None

    def count_text_tokens(self, text: str) -> int:
        if not text:
            return 0
        if self._enc is None:
            return max(1, len(text) // 3)
        return len(self._enc.encode(text))

    def truncate_text_to_tokens_keep_tail(self, text: str, max_tokens: int) -> str:
        if max_tokens <= 0 or not text:
            return ""
        if self._enc is None:
            max_chars = max(1, max_tokens * 3)
            if len(text) <= max_chars:
                return text
            return "‚Ä¶ " + text[-max_chars:]
        toks = self._enc.encode(text)
        if len(toks) <= max_tokens:
            return text
        return "‚Ä¶ " + self._enc.decode(toks[-max_tokens:])

    @staticmethod
    def _iter_text_blocks(content: Any) -> List[str]:
        if isinstance(content, str):
            return [content]
        if isinstance(content, list):
            out: List[str] = []
            for b in content:
                if isinstance(b, dict) and b.get("type") == "text" and isinstance(b.get("text"), str):
                    out.append(b["text"])
            return out
        return []

    @staticmethod
    def _count_images(content: Any) -> int:
        if isinstance(content, list):
            n = 0
            for b in content:
                if not isinstance(b, dict):
                    continue
                if b.get("type") in ("image_url", "telegram_photo"):
                    n += 1
            return n
        return 0

    def estimate_messages(self, messages: List[Dict[str, Any]]) -> int:
        total = TOKENS_PRIMING_OVERHEAD
        for m in messages:
            total += TOKENS_PER_MESSAGE_OVERHEAD
            content = m.get("content")
            for part in self._iter_text_blocks(content):
                total += self.count_text_tokens(part)
            total += self._count_images(content) * IMAGE_TOKEN_ESTIMATE
        return total


token_estimator = TokenEstimator()


# =============================================================================
# BOT STATE (JSON)
# =============================================================================

settings_store = JsonStore(SETTINGS_FILE, default={})
history_store = JsonStore(
    HISTORY_FILE,
    default={},
    rotate_max_bytes=HISTORY_ROTATE_MAX_BYTES,
    rotate_backups=HISTORY_ROTATE_BACKUPS,
)

user_settings: Dict[str, Dict[str, Any]] = settings_store.get()
chat_histories: Dict[str, List[Dict[str, Any]]] = history_store.get()

bot = telebot.TeleBot(API_TOKEN)

# OpenAI client (LM Studio)
try:
    client = OpenAI(base_url=BASE_URL, api_key=OPENAI_API_KEY, timeout=LLM_TIMEOUT_SEC)  # type: ignore[arg-type]
except TypeError:
    client = OpenAI(base_url=BASE_URL, api_key=OPENAI_API_KEY)


# =============================================================================
# CIRCUIT BREAKER
# =============================================================================

class CircuitBreaker:
    def __init__(self, failure_threshold: int, reset_timeout_sec: float) -> None:
        self.failure_threshold = max(1, failure_threshold)
        self.reset_timeout_sec = max(1.0, reset_timeout_sec)
        self._lock = threading.RLock()
        self._fail_streak = 0
        self._opened_at: Optional[float] = None

    def on_success(self) -> None:
        with self._lock:
            self._fail_streak = 0
            self._opened_at = None

    def on_failure(self) -> None:
        with self._lock:
            self._fail_streak += 1
            if self._fail_streak >= self.failure_threshold and self._opened_at is None:
                self._opened_at = time.time()

    def is_open(self) -> bool:
        with self._lock:
            if self._opened_at is None:
                return False
            if (time.time() - self._opened_at) >= self.reset_timeout_sec:
                # half-open: allow one try (reset opened_at but keep fail streak)
                self._opened_at = None
                return False
            return True

    def status(self) -> Dict[str, Any]:
        with self._lock:
            return {
                "open": self._opened_at is not None and (time.time() - self._opened_at) < self.reset_timeout_sec,
                "fail_streak": self._fail_streak,
                "opened_at": self._opened_at,
                "reset_timeout_sec": self.reset_timeout_sec,
                "failure_threshold": self.failure_threshold,
            }


CB = CircuitBreaker(CB_FAILURE_THRESHOLD, CB_RESET_TIMEOUT_SEC)


# =============================================================================
# SAFE OPENAI CALLS (timeouts/retries)
# =============================================================================

def _openai_models_list() -> Any:
    try:
        return client.models.list(timeout=LLM_TIMEOUT_SEC)  # type: ignore[call-arg]
    except TypeError:
        return client.models.list()


def _openai_chat_create(**kwargs: Any) -> Any:
    try:
        return client.chat.completions.create(timeout=LLM_TIMEOUT_SEC, **kwargs)  # type: ignore[call-arg]
    except TypeError:
        return client.chat.completions.create(**kwargs)


def call_with_retries(fn, *, name: str) -> Any:
    if CB.is_open():
        raise RuntimeError("LLM circuit breaker is open (LM Studio temporarily unavailable).")

    last_exc: Optional[Exception] = None
    for attempt in range(LLM_MAX_RETRIES + 1):
        try:
            res = fn()
            CB.on_success()
            return res
        except Exception as e:
            last_exc = e
            CB.on_failure()
            record_error(f"{name} failed: {type(e).__name__}: {e}")
            if attempt >= LLM_MAX_RETRIES:
                break
            sleep_s = LLM_RETRY_BACKOFF_SEC * (2 ** attempt)
            time.sleep(sleep_s)

    assert last_exc is not None
    raise last_exc


# =============================================================================
# SETTINGS / MEMORY
# =============================================================================

DEFAULT_CFG: Dict[str, Any] = {"role": "default", "temperature": 0.7, "memory": []}


def get_settings(user_id: Union[int, str]) -> Dict[str, Any]:
    k = uid(user_id)
    changed = False
    with STATE_LOCK:
        if k not in user_settings or not isinstance(user_settings.get(k), dict):
            user_settings[k] = copy.deepcopy(DEFAULT_CFG)
            changed = True

        for kk, vv in DEFAULT_CFG.items():
            if kk not in user_settings[k]:
                user_settings[k][kk] = copy.deepcopy(vv)
                changed = True

        # –º–∏–≥—Ä–∞—Ü–∏—è: –≤—ã—Ä–µ–∑–∞–ª–∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        if "model" in user_settings[k]:
            user_settings[k].pop("model", None)
            changed = True
        if "model_key" in user_settings[k]:
            user_settings[k].pop("model_key", None)
            changed = True

        if not isinstance(user_settings[k].get("memory"), list):
            user_settings[k]["memory"] = []
            changed = True

        if changed:
            settings_store.save()

        return user_settings[k]


def memory_text_for(user_id: Union[int, str]) -> str:
    s = get_settings(user_id)
    mem = s.get("memory", [])
    if not mem:
        return ""

    safe_items: List[str] = []
    for item in mem:
        if isinstance(item, str) and item.strip():
            safe_items.append(item.strip())

    if not safe_items:
        return ""

    lines = "\n".join(f"- {x}" for x in safe_items[:MAX_MEMORY_ITEMS])
    return "–ü–∞–º—è—Ç—å –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ (—É—á–∏—Ç—ã–≤–∞–π —ç—Ç–æ, –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ):\n" + lines + "\n"


def system_prompt_for(user_id: Union[int, str]) -> str:
    s = get_settings(user_id)
    role_text = ROLES.get(s.get("role", "default"), ROLES["default"])
    mem = memory_text_for(user_id)
    return (
        f"{role_text}\n\n"
        f"{EXECUTION_GUIDE}\n"
        f"{mem}\n"
        f"{RESPONSE_FORMAT_INSTRUCTION}"
    ).strip()


def init_history(user_id: Union[int, str]) -> None:
    k = uid(user_id)
    with STATE_LOCK:
        chat_histories[k] = [{"role": "system", "content": system_prompt_for(k)}]
        history_store.save()


def refresh_system_prompt_in_history(user_id: str) -> None:
    with STATE_LOCK:
        history = chat_histories.get(user_id)
        if not history:
            init_history(user_id)
            return
        if history[0].get("role") != "system":
            history.insert(0, {"role": "system", "content": system_prompt_for(user_id)})
        else:
            history[0]["content"] = system_prompt_for(user_id)
        history_store.save()


def build_photo_caption(user_caption: Optional[str]) -> str:
    cap = (user_caption or "").strip()
    if cap:
        return cap
    return AUTO_IMAGE_DESCRIPTION_5S


# =============================================================================
# TOKEN STATUS
# =============================================================================

def get_token_status(user_id: Union[int, str]) -> TokenStatus:
    k = uid(user_id)
    with STATE_LOCK:
        history = chat_histories.get(k) or []
        used = token_estimator.estimate_messages(history)
    return TokenStatus(used=used, left=TOKEN_LIMIT - used)


# =============================================================================
# LM STUDIO ACTIVE MODEL RESOLVE
# =============================================================================

_MODEL_ID_CACHE: Dict[str, Any] = {"value": "local-model", "ts": 0.0}


def resolve_lmstudio_model_id() -> str:
    now = time.time()
    if MODEL_ID_TTL_SEC > 0 and (now - float(_MODEL_ID_CACHE["ts"])) < MODEL_ID_TTL_SEC:
        v = _MODEL_ID_CACHE["value"]
        return v if isinstance(v, str) and v else "local-model"

    def _list():
        return _openai_models_list()

    model_id = "local-model"
    try:
        models = call_with_retries(_list, name="models.list")
        data = getattr(models, "data", None)
        if isinstance(data, list) and data:
            mid = getattr(data[0], "id", None)
            if isinstance(mid, str) and mid.strip():
                model_id = mid.strip()
    except Exception:
        # leave fallback
        pass

    _MODEL_ID_CACHE["value"] = model_id
    _MODEL_ID_CACHE["ts"] = now
    return model_id


# =============================================================================
# STORAGE (NO BASE64 IN JSON)
# =============================================================================

def store_user_text(user_id: str, text: str, job_id: int) -> None:
    with STATE_LOCK:
        chat_histories[user_id].append({"role": "user", "content": [{"type": "text", "text": text}], "_job_id": job_id})
        history_store.save()


def store_user_photo(user_id: str, file_id: str, caption: str, job_id: int) -> None:
    with STATE_LOCK:
        chat_histories[user_id].append(
            {"role": "user", "content": [{"type": "telegram_photo", "file_id": file_id, "caption": caption}], "_job_id": job_id}
        )
        history_store.save()


def insert_assistant_after_job(user_id: str, job_id: int, text: str) -> bool:
    with STATE_LOCK:
        history = chat_histories.get(user_id)
        if not history:
            return False

        idx = None
        for i, m in enumerate(history):
            if m.get("role") == "user" and m.get("_job_id") == job_id:
                idx = i
                break
        if idx is None:
            return False

        history[idx].pop("_job_id", None)
        history.insert(idx + 1, {"role": "assistant", "content": text})
        history_store.save()
        return True


def remove_user_message_by_job(user_id: str, job_id: int) -> bool:
    with STATE_LOCK:
        history = chat_histories.get(user_id)
        if not history:
            return False
        for i, m in enumerate(history):
            if m.get("role") == "user" and m.get("_job_id") == job_id:
                if i + 1 < len(history) and history[i + 1].get("role") == "assistant":
                    return False
                history.pop(i)
                history_store.save()
                return True
        return False


def materialize_for_api(history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []

    for msg in history:
        role = msg.get("role")
        content = msg.get("content")

        if isinstance(content, str) or content is None:
            out.append({"role": role, "content": content or ""})
            continue

        if isinstance(content, list):
            blocks: List[Dict[str, Any]] = []
            for b in content:
                if not isinstance(b, dict):
                    continue
                t = b.get("type")

                if t == "text":
                    text = b.get("text")
                    if isinstance(text, str) and text.strip():
                        blocks.append({"type": "text", "text": text})

                elif t == "image_url":
                    blocks.append(b)

                elif t == "telegram_photo":
                    file_id = b.get("file_id")
                    caption = b.get("caption") if isinstance(b.get("caption"), str) else ""
                    if isinstance(file_id, str) and file_id:
                        try:
                            file_info = bot.get_file(file_id)
                            downloaded = bot.download_file(file_info.file_path)
                            b64 = base64.b64encode(downloaded).decode("utf-8")
                            blocks.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
                        except Exception as e:
                            record_error(f"materialize photo failed: {type(e).__name__}: {e}")
                            blocks.append({"type": "text", "text": "[–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ]"})
                    if caption.strip():
                        blocks.append({"type": "text", "text": caption})

            out.append({"role": role, "content": blocks if blocks else ""})
            continue

        out.append({"role": role, "content": str(content)})

    return out


# =============================================================================
# DEDUP + RATE LIMIT
# =============================================================================

_dedup_lock = threading.RLock()
_seen_msgs: Deque[Tuple[int, int, float]] = deque(maxlen=DEDUP_CACHE_SIZE)  # (chat_id, msg_id, ts)
_seen_set: set[Tuple[int, int]] = set()


def is_duplicate_message(chat_id: int, message_id: int) -> bool:
    now = time.time()
    key = (chat_id, message_id)
    with _dedup_lock:
        # cleanup TTL
        while _seen_msgs and (now - _seen_msgs[0][2]) > DEDUP_TTL_SEC:
            old_chat, old_mid, _ts = _seen_msgs.popleft()
            _seen_set.discard((old_chat, old_mid))

        if key in _seen_set:
            return True
        _seen_set.add(key)
        _seen_msgs.append((chat_id, message_id, now))
        return False


_rate_lock = threading.RLock()
_user_last_ts: Dict[str, float] = {}
_user_window: Dict[str, Deque[float]] = defaultdict(lambda: deque(maxlen=200))


def check_rate_limit(user_id: str) -> Optional[str]:
    now = time.time()
    with _rate_lock:
        last = _user_last_ts.get(user_id)
        if last is not None and (now - last) < USER_MIN_INTERVAL_SEC:
            return f"–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏ {USER_MIN_INTERVAL_SEC:.0f} —Å–µ–∫."

        window = _user_window[user_id]
        # cleanup 60s
        while window and (now - window[0]) > 60:
            window.popleft()
        if len(window) >= USER_MAX_PER_MINUTE:
            return f"–õ–∏–º–∏—Ç: {USER_MAX_PER_MINUTE}/–º–∏–Ω. –ü–æ–¥–æ–∂–¥–∏ –Ω–µ–º–Ω–æ–≥–æ."

        _user_last_ts[user_id] = now
        window.append(now)
        return None


# =============================================================================
# QUEUE / JOBS
# =============================================================================

@dataclass
class Job:
    job_id: int
    user_id: str
    chat_id: int
    status_message_id: int
    created_at: float
    priority: int
    has_image: bool
    cancel_event: threading.Event = field(default_factory=threading.Event)
    started: bool = False
    done: bool = False
    canceled: bool = False


jobs: Dict[int, Job] = {}
user_queues: Dict[str, Deque[int]] = {}
user_busy: Dict[str, bool] = {}
active_job_by_user: Dict[str, int] = {}
active_global: int = 0


def compute_priority(user_id: str, prompt_tokens_estimate: int, used_tokens_estimate: int, has_image: bool) -> int:
    is_owner = False
    try:
        is_owner = (int(user_id) == BOT_OWNER_ID and BOT_OWNER_ID != 0)
    except Exception:
        is_owner = False

    if is_owner:
        return 2_000_000_000

    pr = 50_000
    pr -= prompt_tokens_estimate * TOKENS_PRIORITY_WEIGHT
    pr -= used_tokens_estimate * USED_TOKENS_WEIGHT
    if has_image:
        pr -= IMAGE_PRIORITY_PENALTY
    return pr


def get_or_create_user_queue(user_id: str) -> Deque[int]:
    q = user_queues.get(user_id)
    if q is None:
        q = deque()
        user_queues[user_id] = q
    user_busy.setdefault(user_id, False)
    return q


def enqueue_job(job: Job) -> bool:
    with SCHED_LOCK:
        q = get_or_create_user_queue(job.user_id)

        # clean head garbage
        while q:
            j = jobs.get(q[0])
            if not j or j.canceled or j.done:
                q.popleft()
            else:
                break

        if len(q) >= MAX_PENDING_PER_USER:
            return False

        q.append(job.job_id)
        jobs[job.job_id] = job
        SCHED_COND.notify_all()
        return True


def select_next_job_id() -> Optional[int]:
    global active_global
    with SCHED_LOCK:
        if active_global >= MAX_ACTIVE_GLOBAL:
            return None

        best: Optional[Tuple[int, float, int]] = None  # (priority, created_at, job_id)

        for u, q in user_queues.items():
            if not q:
                continue
            if user_busy.get(u, False):
                continue

            while q:
                j = jobs.get(q[0])
                if not j or j.canceled or j.done:
                    q.popleft()
                else:
                    break
            if not q:
                continue

            jid = q[0]
            j = jobs.get(jid)
            if not j or j.canceled or j.done:
                q.popleft()
                continue

            cand = (j.priority, j.created_at, jid)
            if best is None or cand[0] > best[0] or (cand[0] == best[0] and cand[1] < best[1]):
                best = cand

        if best is None:
            return None

        jid = best[2]
        j = jobs.get(jid)
        if not j:
            return None

        q = user_queues.get(j.user_id)
        if q and q[0] == jid:
            q.popleft()
        elif q:
            try:
                q.remove(jid)
            except ValueError:
                pass

        user_busy[j.user_id] = True
        active_job_by_user[j.user_id] = jid
        active_global += 1
        return jid


def mark_job_finished(user_id: str, job_id: int) -> None:
    global active_global
    with SCHED_LOCK:
        user_busy[user_id] = False
        if active_job_by_user.get(user_id) == job_id:
            active_job_by_user.pop(user_id, None)
        if active_global > 0:
            active_global -= 1
        SCHED_COND.notify_all()


def has_pending_for_user(user_id: str) -> bool:
    with SCHED_LOCK:
        q = user_queues.get(user_id)
        if not q:
            return False
        for jid in q:
            j = jobs.get(jid)
            if j and not j.canceled and not j.done:
                return True
        return False


def cleanup_jobs(max_keep: int = 5000) -> None:
    with SCHED_LOCK:
        if len(jobs) <= max_keep:
            return
        done_ids = [jid for jid, j in jobs.items() if j.done or j.canceled]
        done_ids.sort(key=lambda jid: jobs[jid].created_at if jid in jobs else 0.0)
        to_delete = done_ids[: max(0, len(jobs) - max_keep)]
        for jid in to_delete:
            jobs.pop(jid, None)


# =============================================================================
# QUEUE STATUS
# =============================================================================

@dataclass(frozen=True)
class QueueStatus:
    user_ahead: int
    user_position: int
    user_has_active: bool
    global_pending: int
    global_active: int
    priority: int


def compute_queue_status_for_job(user_id: str, job_id: int) -> QueueStatus:
    with SCHED_LOCK:
        q = user_queues.get(user_id) or deque()
        busy = bool(user_busy.get(user_id, False))

        global_pending = sum(len(qq) for qq in user_queues.values())
        global_active_now = active_global

        job = jobs.get(job_id)
        pr = job.priority if job else 0

        idx_in_q = -1
        try:
            idx_in_q = list(q).index(job_id)
        except ValueError:
            idx_in_q = -1

        ahead_in_q = idx_in_q if idx_in_q >= 0 else 0
        user_ahead = ahead_in_q + (1 if busy else 0)
        user_position = (idx_in_q + 1) + (1 if busy else 0) if idx_in_q >= 0 else (1 if busy else 1)

        return QueueStatus(
            user_ahead=user_ahead,
            user_position=user_position,
            user_has_active=busy,
            global_pending=global_pending,
            global_active=global_active_now,
            priority=pr,
        )


# =============================================================================
# UI (KEYBOARDS)
# =============================================================================

def main_menu_keyboard(user_id: Union[int, str]) -> types.InlineKeyboardMarkup:
    st = get_token_status(user_id)
    s = get_settings(user_id)

    markup = types.InlineKeyboardMarkup(row_width=2)
    markup.add(types.InlineKeyboardButton(f"üß† Tokens: {st.used}/{TOKEN_LIMIT}", callback_data="show_tokens"))
    markup.add(types.InlineKeyboardButton("üóëÔ∏è –ù–æ–≤—ã–π —á–∞—Ç", callback_data="new_chat"))
    markup.add(types.InlineKeyboardButton(f"üé≠ –†–æ–ª—å: {s['role']}", callback_data="menu_roles"))
    markup.add(types.InlineKeyboardButton(f"üå°Ô∏è Temp: {s['temperature']}", callback_data="menu_temp"))
    markup.add(types.InlineKeyboardButton("üßæ –ü–∞–º—è—Ç—å", callback_data="menu_memory"))
    markup.add(types.InlineKeyboardButton("üìå –û—á–µ—Ä–µ–¥—å", callback_data="show_queue"))
    return markup


def roles_keyboard(user_id: Union[int, str]) -> types.InlineKeyboardMarkup:
    current = get_settings(user_id)["role"]
    markup = types.InlineKeyboardMarkup()
    for r in ROLES.keys():
        mark = " ‚úÖ" if r == current else ""
        markup.add(types.InlineKeyboardButton(f"üé≠ {r}{mark}", callback_data=f"set_role_{r}"))
    markup.add(types.InlineKeyboardButton("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="main_menu"))
    return markup


def temp_keyboard(user_id: Union[int, str]) -> types.InlineKeyboardMarkup:
    current = float(get_settings(user_id)["temperature"])
    markup = types.InlineKeyboardMarkup()
    for t in ["0.1", "0.3", "0.7", "1.0"]:
        mark = " ‚úÖ" if float(t) == current else ""
        markup.add(types.InlineKeyboardButton(f"üå°Ô∏è {t}{mark}", callback_data=f"set_temp_{t}"))
    markup.add(types.InlineKeyboardButton("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="main_menu"))
    return markup


def stop_keyboard(job_id: int) -> types.InlineKeyboardMarkup:
    markup = types.InlineKeyboardMarkup()
    markup.add(types.InlineKeyboardButton("üõë –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", callback_data=f"stop:{job_id}"))
    return markup


def memory_keyboard() -> types.InlineKeyboardMarkup:
    markup = types.InlineKeyboardMarkup(row_width=2)
    markup.add(types.InlineKeyboardButton("üßπ –û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å", callback_data="memory_clear"))
    markup.add(types.InlineKeyboardButton("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="main_menu"))
    return markup


# =============================================================================
# TELEGRAM UTILS
# =============================================================================

def safe_delete(chat_id: int, message_id: int) -> None:
    try:
        bot.delete_message(chat_id, message_id)
    except Exception:
        pass


def safe_edit_text(chat_id: int, message_id: int, text: str, reply_markup: Optional[Any] = None) -> None:
    try:
        bot.edit_message_text(text, chat_id, message_id, reply_markup=reply_markup)
    except Exception:
        try:
            bot.send_message(chat_id, text, reply_markup=reply_markup)
        except Exception:
            pass


def send_long_message(chat_id: int, text: str, reply_markup: Optional[Any] = None) -> None:
    max_len = 3900
    chunks = [text[i:i + max_len] for i in range(0, len(text), max_len)] or [""]
    for i, chunk in enumerate(chunks):
        bot.send_message(chat_id, chunk, reply_markup=reply_markup if i == len(chunks) - 1 else None)


# =============================================================================
# SUMMARY / COMPRESSION (kept)
# =============================================================================

def _is_summary_msg(msg: Dict[str, Any]) -> bool:
    return msg.get("role") == "system" and isinstance(msg.get("content"), str) and msg["content"].startswith("[SUMMARY]")


def _is_ultra_msg(msg: Dict[str, Any]) -> bool:
    return msg.get("role") == "system" and isinstance(msg.get("content"), str) and msg["content"].startswith("[ULTRA]")


def _content_to_plain_text(msg: Dict[str, Any]) -> str:
    content = msg.get("content")
    if isinstance(content, str):
        return content.strip()
    if isinstance(content, list):
        parts: List[str] = []
        for b in content:
            if not isinstance(b, dict):
                continue
            if b.get("type") == "text" and isinstance(b.get("text"), str):
                parts.append(b["text"])
            elif b.get("type") in ("image_url", "telegram_photo"):
                parts.append("[–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ]")
        return " ".join(p.strip() for p in parts if p).strip()
    return ""


def compress_summary(history_slice: List[Dict[str, Any]]) -> str:
    parts: List[str] = []
    for msg in history_slice:
        role = msg.get("role")
        if role not in ("user", "assistant"):
            continue
        text = _content_to_plain_text(msg)
        if not text:
            continue
        parts.append(("U: " if role == "user" else "A: ") + text)
    return " | ".join(parts)


def compression_engine_inplace(user_id: str) -> None:
    with STATE_LOCK:
        history = chat_histories.get(user_id)
        if not history:
            chat_histories[user_id] = [{"role": "system", "content": system_prompt_for(user_id)}]
            history_store.save()
            return

        # refresh system
        if history[0].get("role") == "system":
            history[0]["content"] = system_prompt_for(user_id)
        else:
            history.insert(0, {"role": "system", "content": system_prompt_for(user_id)})

        has_sum = any(_is_summary_msg(m) for m in history)
        has_ultra = any(_is_ultra_msg(m) for m in history)

        if len(history) > 12 and not has_sum and not has_ultra:
            window = history[1:9]
            summary = compress_summary(window)
            history = [history[0], {"role": "system", "content": f"[SUMMARY] {summary}"}] + history[9:]
            chat_histories[user_id] = history

        history = chat_histories[user_id]
        if len(history) > 18 and not any(_is_ultra_msg(m) for m in history):
            for i, m in enumerate(history):
                if _is_summary_msg(m):
                    compact = str(m["content"]).replace("[SUMMARY] ", "")
                    if len(compact) > 240:
                        compact = compact[:240].rstrip() + "‚Ä¶"
                    history[i] = {"role": "system", "content": f"[ULTRA] {compact}"}
                    break

        history_store.save()


# =============================================================================
# STRICT TOKEN BUDGET (kept)
# =============================================================================

def _extract_summary_msg(history: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    for m in history[1:]:
        if _is_ultra_msg(m) or _is_summary_msg(m):
            return m
    return None


def _non_system_msgs(history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    return [m for m in history if m.get("role") != "system"]


def _rebuild_history(sys0: Dict[str, Any], summary: Optional[Dict[str, Any]], tail: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out = [sys0]
    if summary is not None:
        out.append(summary)
    out.extend(tail)
    return out


def enforce_token_budget_strict_list(user_id: str, history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not history:
        return [{"role": "system", "content": system_prompt_for(user_id)}]

    history = copy.deepcopy(history)
    if history[0].get("role") == "system":
        history[0]["content"] = system_prompt_for(user_id)
    else:
        history.insert(0, {"role": "system", "content": system_prompt_for(user_id)})

    for m in history:
        m.pop("_job_id", None)

    sys0 = history[0]
    summary = _extract_summary_msg(history)
    tail = _non_system_msgs(history)

    candidate = _rebuild_history(sys0, summary, tail)
    while len(tail) > 1 and token_estimator.estimate_messages(candidate) > TOKEN_LIMIT:
        tail = tail[1:]
        candidate = _rebuild_history(sys0, summary, tail)

    if token_estimator.estimate_messages(candidate) > TOKEN_LIMIT and tail:
        last = copy.deepcopy(tail[-1])
        content = last.get("content")

        def get_joined_text(c: Any) -> Optional[str]:
            if isinstance(c, str):
                return c
            if isinstance(c, list):
                texts = []
                for b in c:
                    if isinstance(b, dict) and b.get("type") == "text" and isinstance(b.get("text"), str):
                        texts.append(b["text"])
                joined = " ".join(t.strip() for t in texts if t and isinstance(t, str)).strip()
                return joined if joined else None
            return None

        joined_text = get_joined_text(content)
        if joined_text:
            last_copy = copy.deepcopy(last)
            if isinstance(content, str):
                last_copy["content"] = ""
            elif isinstance(content, list):
                kept = []
                for b in content:
                    if isinstance(b, dict) and b.get("type") in ("image_url", "telegram_photo"):
                        nb = dict(b)
                        if nb.get("type") == "telegram_photo":
                            nb.pop("caption", None)
                        kept.append(nb)
                last_copy["content"] = kept
            else:
                last_copy["content"] = ""

            base_candidate = _rebuild_history(sys0, summary, tail[:-1] + [last_copy])
            base_tokens = token_estimator.estimate_messages(base_candidate)
            allowance = max(0, TOKEN_LIMIT - base_tokens)
            allowance = max(allowance, MIN_TEXT_TOKENS_TO_KEEP)
            truncated = token_estimator.truncate_text_to_tokens_keep_tail(joined_text, allowance)

            if isinstance(content, str):
                last["content"] = truncated
            elif isinstance(content, list):
                new_blocks: List[Dict[str, Any]] = []
                for b in content:
                    if isinstance(b, dict) and b.get("type") == "telegram_photo":
                        nb = dict(b)
                        nb.pop("caption", None)
                        new_blocks.append(nb)
                new_blocks.append({"type": "text", "text": truncated})
                last["content"] = new_blocks
            else:
                last["content"] = truncated

            tail[-1] = last
            candidate = _rebuild_history(sys0, summary, tail)

    return candidate


# =============================================================================
# SNAPSHOT FOR JOB
# =============================================================================

def find_job_user_message_index(history: List[Dict[str, Any]], job_id: int) -> Optional[int]:
    for i, m in enumerate(history):
        if m.get("role") == "user" and m.get("_job_id") == job_id:
            return i
    return None


def snapshot_history_for_job(user_id: str, job_id: int) -> List[Dict[str, Any]]:
    with STATE_LOCK:
        history = chat_histories.get(user_id) or [{"role": "system", "content": system_prompt_for(user_id)}]
        idx = find_job_user_message_index(history, job_id)
        snap = copy.deepcopy(history[: idx + 1]) if idx is not None else copy.deepcopy(history)

    if snap and snap[0].get("role") == "system":
        snap[0]["content"] = system_prompt_for(user_id)
    else:
        snap.insert(0, {"role": "system", "content": system_prompt_for(user_id)})

    for m in snap:
        m.pop("_job_id", None)

    return enforce_token_budget_strict_list(user_id, snap)


def message_has_image(message: types.Message) -> bool:
    return bool(message.content_type == "photo" and message.photo)


# =============================================================================
# COMPLETION (STREAMING + STOP SUPPORT + retries)
# =============================================================================

def run_completion_streaming(
    api_messages: List[Dict[str, Any]],
    temperature: float,
    cancel_event: threading.Event,
) -> str:
    model_id = resolve_lmstudio_model_id()
    chunks: List[str] = []

    def _stream_call():
        return _openai_chat_create(
            model=model_id,
            messages=api_messages,
            temperature=temperature,
            stream=True,
        )

    def _nonstream_call():
        return _openai_chat_create(
            model=model_id,
            messages=api_messages,
            temperature=temperature,
        )

    # Prefer streaming, fallback to non-stream.
    try:
        stream = call_with_retries(_stream_call, name="chat.create(stream)")
        for ev in stream:
            if cancel_event.is_set():
                break
            delta = None
            try:
                delta = ev.choices[0].delta.content  # type: ignore[attr-defined]
            except Exception:
                delta = None
            if isinstance(delta, str) and delta:
                chunks.append(delta)
        return "".join(chunks)
    except Exception as e:
        record_error(f"stream failed -> fallback non-stream: {type(e).__name__}: {e}")

    completion = call_with_retries(_nonstream_call, name="chat.create")
    return completion.choices[0].message.content or ""


# =============================================================================
# POSTPROCESS (ONLY WHEN IDLE)
# =============================================================================

def postprocess_user_history_if_idle(user_id: str) -> None:
    with SCHED_LOCK:
        busy = user_busy.get(user_id, False)
        pending = has_pending_for_user(user_id)
    if busy or pending:
        return

    refresh_system_prompt_in_history(user_id)
    compression_engine_inplace(user_id)

    with STATE_LOCK:
        current = chat_histories.get(user_id) or [{"role": "system", "content": system_prompt_for(user_id)}]
        trimmed = enforce_token_budget_strict_list(user_id, current)
        chat_histories[user_id] = trimmed
        history_store.save()


# =============================================================================
# WORKER THREADS
# =============================================================================

def worker_loop(worker_id: int) -> None:
    logger.info("Worker #%d started", worker_id)

    while not SHUTDOWN_EVENT.is_set():
        with SCHED_LOCK:
            jid = select_next_job_id()
            if jid is None:
                SCHED_COND.wait(timeout=1.0)
                continue

        job = jobs.get(jid)
        if not job:
            continue

        job.started = True

        with STATE_LOCK:
            s = get_settings(job.user_id)
            temperature = float(s["temperature"])

        safe_edit_text(
            job.chat_id,
            job.status_message_id,
            "‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç‚Ä¶ (–º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–Ω–æ–ø–∫–æ–π –Ω–∏–∂–µ)",
            reply_markup=stop_keyboard(job.job_id),
        )

        try:
            snap = snapshot_history_for_job(job.user_id, job.job_id)
            api_messages = materialize_for_api(snap)

            raw = run_completion_streaming(
                api_messages=api_messages,
                temperature=temperature,
                cancel_event=job.cancel_event,
            )

            canceled = job.cancel_event.is_set() or SHUTDOWN_EVENT.is_set()

            if canceled and SMART_STOP_DISCARD_PARTIAL:
                response = "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ."
            else:
                response = raw or ""
                if "–û–¢–í–ï–¢:" in response:
                    response = response.split("–û–¢–í–ï–¢:", 1)[1].strip()
                if canceled and not response.strip():
                    response = "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ."

            inserted = insert_assistant_after_job(job.user_id, job.job_id, response)
            if not inserted:
                job.canceled = True

            safe_delete(job.chat_id, job.status_message_id)
            send_long_message(job.chat_id, response, reply_markup=main_menu_keyboard(job.user_id))

        except Exception as e:
            record_error(f"worker error: {type(e).__name__}: {e}")
            safe_delete(job.chat_id, job.status_message_id)
            bot.send_message(job.chat_id, f"–û—à–∏–±–∫–∞: {e}", reply_markup=main_menu_keyboard(job.user_id))
        finally:
            job.done = True
            mark_job_finished(job.user_id, job.job_id)
            postprocess_user_history_if_idle(job.user_id)
            cleanup_jobs()


_workers: List[threading.Thread] = []
for i in range(max(1, WORKER_COUNT)):
    t = threading.Thread(target=worker_loop, args=(i + 1,), daemon=True)
    _workers.append(t)
    t.start()


# =============================================================================
# COMMANDS
# =============================================================================

@bot.message_handler(commands=["start"])
def cmd_start(message: types.Message) -> None:
    user_id = uid(message.from_user.id)
    with STATE_LOCK:
        if user_id not in chat_histories:
            init_history(user_id)
    refresh_system_prompt_in_history(user_id)
    bot.reply_to(message, "‚öôÔ∏è –ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è:", reply_markup=main_menu_keyboard(user_id))


@bot.message_handler(commands=["export"])
def cmd_export(message: types.Message) -> None:
    user_id = uid(message.from_user.id)
    with STATE_LOCK:
        if user_id not in chat_histories:
            init_history(user_id)

        payload = {
            "user_id": user_id,
            "settings": get_settings(user_id),
            "token_status_estimate": {
                "used": get_token_status(user_id).used,
                "left": get_token_status(user_id).left,
                "limit": TOKEN_LIMIT,
            },
            "lmstudio_loaded_model_id_estimate": resolve_lmstudio_model_id(),
            "history": chat_histories.get(user_id, []),
        }

    data = json.dumps(payload, ensure_ascii=False, indent=2).encode("utf-8")
    bio = io.BytesIO(data)
    bio.name = f"export_{user_id}.json"
    bot.send_document(message.chat.id, bio, caption="üì¶ –≠–∫—Å–ø–æ—Ä—Ç –∏—Å—Ç–æ—Ä–∏–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ (JSON).")


@bot.message_handler(commands=["profile"])
def cmd_profile(message: types.Message) -> None:
    user_id = uid(message.from_user.id)
    s = get_settings(user_id)
    mem = s.get("memory", [])
    lines = [
        f"üë§ User: {user_id}",
        f"üé≠ Role: {s.get('role')}",
        f"üå°Ô∏è Temp: {s.get('temperature')}",
        f"ü§ñ LM Studio model: {resolve_lmstudio_model_id()}",
        "",
        "üßæ –ü–∞–º—è—Ç—å:",
    ]
    if mem:
        for i, item in enumerate(mem[:MAX_MEMORY_ITEMS], 1):
            lines.append(f"{i}) {item}")
    else:
        lines.append("(–ø—É—Å—Ç–æ)")
    lines.append("")
    lines.append("–ö–æ–º–∞–Ω–¥—ã: /remember <—Ç–µ–∫—Å—Ç>, /forget <n|all>, /queue, /stop, /status")
    bot.send_message(message.chat.id, "\n".join(lines), reply_markup=main_menu_keyboard(user_id))


@bot.message_handler(commands=["status"])
def cmd_status(message: types.Message) -> None:
    # owner-only
    if int(message.from_user.id) != BOT_OWNER_ID:
        return

    uptime = time.time() - START_TS
    cb = CB.status()

    with SCHED_LOCK:
        global_pending = sum(len(qq) for qq in user_queues.values())
        global_active_now = active_global
        users_in_queue = sum(1 for qq in user_queues.values() if len(qq) > 0)
        active_users = sum(1 for v in user_busy.values() if v)

    last_errs = list(RECENT_ERRORS)[-8:]
    err_text = "\n".join(
        f"- {time.strftime('%H:%M:%S', time.localtime(ts))}: {msg}" for ts, msg in last_errs
    ) or "(–Ω–µ—Ç)"

    text = (
        "üõ† /status\n"
        f"‚è± Uptime: {uptime:.0f}s\n"
        f"ü§ñ LM Studio model: {resolve_lmstudio_model_id()}\n"
        f"‚öôÔ∏è Active(global): {global_active_now}/{MAX_ACTIVE_GLOBAL} | workers={WORKER_COUNT}\n"
        f"üì• Pending(global): {global_pending} | users_in_queue={users_in_queue}\n"
        f"üë• Active users: {active_users}\n"
        "\n"
        f"üßØ Circuit breaker: open={cb['open']} fail_streak={cb['fail_streak']} "
        f"threshold={cb['failure_threshold']} reset={cb['reset_timeout_sec']}s\n"
        "\n"
        "‚ùó –ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏:\n"
        f"{err_text}"
    )
    bot.reply_to(message, text, reply_markup=main_menu_keyboard(uid(message.from_user.id)))


@bot.message_handler(commands=["remember"])
def cmd_remember(message: types.Message) -> None:
    user_id = uid(message.from_user.id)
    text = (message.text or "").strip()
    parts = text.split(maxsplit=1)
    if len(parts) < 2 or not parts[1].strip():
        bot.reply_to(message, "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /remember <—á—Ç–æ –∑–∞–ø–æ–º–Ω–∏—Ç—å>", reply_markup=main_menu_keyboard(user_id))
        return

    item = parts[1].strip()
    if len(item) > MAX_MEMORY_ITEM_LEN:
        item = item[:MAX_MEMORY_ITEM_LEN].rstrip() + "‚Ä¶"

    with STATE_LOCK:
        s = get_settings(user_id)
        mem = s.setdefault("memory", [])
        if not isinstance(mem, list):
            mem = []
            s["memory"] = mem
        mem = [m for m in mem if isinstance(m, str) and m.strip() and m.strip() != item]
        mem.insert(0, item)
        s["memory"] = mem[:MAX_MEMORY_ITEMS]
        settings_store.save()

    refresh_system_prompt_in_history(user_id)
    bot.reply_to(message, "‚úÖ –ó–∞–ø–æ–º–Ω–∏–ª.", reply_markup=main_menu_keyboard(user_id))


@bot.message_handler(commands=["forget"])
def cmd_forget(message: types.Message) -> None:
    user_id = uid(message.from_user.id)
    text = (message.text or "").strip()
    parts = text.split(maxsplit=1)

    with STATE_LOCK:
        s = get_settings(user_id)
        mem = s.get("memory", [])
        if not isinstance(mem, list):
            mem = []
            s["memory"] = mem

        if len(parts) < 2:
            if not mem:
                bot.reply_to(message, "–ü–∞–º—è—Ç—å –ø—É—Å—Ç–∞—è.", reply_markup=main_menu_keyboard(user_id))
                return
            lines = ["üßæ –ü–∞–º—è—Ç—å:"]
            for i, item in enumerate(mem[:MAX_MEMORY_ITEMS], 1):
                lines.append(f"{i}) {item}")
            lines.append("–£–¥–∞–ª–µ–Ω–∏–µ: /forget <–Ω–æ–º–µ—Ä> –∏–ª–∏ /forget all")
            bot.reply_to(message, "\n".join(lines), reply_markup=main_menu_keyboard(user_id))
            return

        arg = parts[1].strip().lower()
        if arg == "all":
            s["memory"] = []
            settings_store.save()
        else:
            try:
                n = int(arg)
                if n < 1 or n > len(mem):
                    bot.reply_to(message, "–ù–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä.", reply_markup=main_menu_keyboard(user_id))
                    return
                mem.pop(n - 1)
                s["memory"] = mem
                settings_store.save()
            except ValueError:
                bot.reply_to(message, "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /forget <–Ω–æ–º–µ—Ä> –∏–ª–∏ /forget all", reply_markup=main_menu_keyboard(user_id))
                return

    refresh_system_prompt_in_history(user_id)
    bot.reply_to(message, "‚úÖ –ì–æ—Ç–æ–≤–æ.", reply_markup=main_menu_keyboard(user_id))


@bot.message_handler(commands=["stop"])
def cmd_stop(message: types.Message) -> None:
    user_id = uid(message.from_user.id)
    with SCHED_LOCK:
        jid = active_job_by_user.get(user_id)
    if not jid:
        bot.reply_to(message, "–°–µ–π—á–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.", reply_markup=main_menu_keyboard(user_id))
        return

    j = jobs.get(jid)
    if j and not j.done and not j.canceled:
        j.cancel_event.set()
    bot.reply_to(message, "üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é‚Ä¶", reply_markup=main_menu_keyboard(user_id))


@bot.message_handler(commands=["queue"])
def cmd_queue(message: types.Message) -> None:
    user_id = uid(message.from_user.id)
    with SCHED_LOCK:
        q = user_queues.get(user_id) or deque()
        busy = bool(user_busy.get(user_id, False))
        active_id = active_job_by_user.get(user_id)
        global_pending = sum(len(qq) for qq in user_queues.values())
        global_active_now = active_global

    lines = [
        "üìå –û—á–µ—Ä–µ–¥—å",
        f"‚öôÔ∏è Active(global): {global_active_now}/{MAX_ACTIVE_GLOBAL}",
        f"üì• Pending(global): {global_pending}",
        "",
        f"üë§ –£ —Ç–µ–±—è –∞–∫—Ç–∏–≤–Ω–∞—è: {'–¥–∞' if busy else '–Ω–µ—Ç'}" + (f" (job {active_id})" if active_id else ""),
        f"üì¨ Pending —É —Ç–µ–±—è: {len(q)}",
    ]
    if q:
        lines.append("–¢–≤–æ–∏ pending job_id: " + ", ".join(str(x) for x in list(q)[:10]) + ("‚Ä¶" if len(q) > 10 else ""))
    bot.send_message(message.chat.id, "\n".join(lines), reply_markup=main_menu_keyboard(user_id))


# =============================================================================
# CALLBACKS
# =============================================================================

@bot.callback_query_handler(func=lambda call: True)
def callback_handler(call: types.CallbackQuery) -> None:
    user_id = uid(call.from_user.id)
    s = get_settings(user_id)

    if call.data == "main_menu":
        safe_edit_text(call.message.chat.id, call.message.message_id, "‚öôÔ∏è –ú–µ–Ω—é:", reply_markup=main_menu_keyboard(user_id))

    elif call.data == "new_chat":
        with SCHED_LOCK:
            busy = user_busy.get(user_id, False)
            pending = has_pending_for_user(user_id)
        if busy or pending:
            bot.send_message(call.message.chat.id, "–°–Ω–∞—á–∞–ª–∞ –¥–æ–∂–¥–∏—Å—å/–æ—Å—Ç–∞–Ω–æ–≤–∏ —Ç–µ–∫—É—â–∏–µ –∑–∞–¥–∞—á–∏.", reply_markup=main_menu_keyboard(user_id))
            return
        init_history(user_id)
        bot.send_message(call.message.chat.id, "üßπ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞.", reply_markup=main_menu_keyboard(user_id))

    elif call.data == "menu_roles":
        safe_edit_text(call.message.chat.id, call.message.message_id, "üé≠ –í—ã–±–µ—Ä–∏—Ç–µ —Ä–æ–ª—å:", reply_markup=roles_keyboard(user_id))

    elif call.data.startswith("set_role_"):
        role = call.data.replace("set_role_", "", 1)
        if role not in ROLES:
            bot.answer_callback_query(call.id, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ä–æ–ª—å.")
            return
        with STATE_LOCK:
            s["role"] = role
            settings_store.save()
        refresh_system_prompt_in_history(user_id)
        safe_edit_text(call.message.chat.id, call.message.message_id, "‚úÖ –†–æ–ª—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∞.", reply_markup=main_menu_keyboard(user_id))

    elif call.data == "menu_temp":
        safe_edit_text(call.message.chat.id, call.message.message_id, "üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞:", reply_markup=temp_keyboard(user_id))

    elif call.data.startswith("set_temp_"):
        with STATE_LOCK:
            try:
                s["temperature"] = float(call.data.replace("set_temp_", "", 1))
            except ValueError:
                s["temperature"] = DEFAULT_CFG["temperature"]
            settings_store.save()
        safe_edit_text(call.message.chat.id, call.message.message_id, "‚úÖ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!", reply_markup=main_menu_keyboard(user_id))

    elif call.data == "show_tokens":
        st = get_token_status(user_id)
        cb = CB.status()
        bot.send_message(
            call.message.chat.id,
            f"üß† Tokens (–æ—Ü–µ–Ω–∫–∞): {st.used}/{TOKEN_LIMIT}\n"
            f"üìâ –û—Å—Ç–∞–ª–æ—Å—å (–æ—Ü–µ–Ω–∫–∞): {st.left}\n"
            f"ü§ñ LM Studio model: {resolve_lmstudio_model_id()}\n"
            f"üßØ CB open={cb['open']} fail_streak={cb['fail_streak']}\n"
            f"üì¶ –≠–∫—Å–ø–æ—Ä—Ç: /export\n"
            f"üìå –û—á–µ—Ä–µ–¥—å: /queue",
            reply_markup=main_menu_keyboard(user_id),
        )

    elif call.data == "show_queue":
        cmd_queue(call.message)

    elif call.data == "menu_memory":
        mem = get_settings(user_id).get("memory", [])
        lines = ["üßæ –ü–∞–º—è—Ç—å:"]
        if mem:
            for i, item in enumerate(mem[:MAX_MEMORY_ITEMS], 1):
                lines.append(f"{i}) {item}")
        else:
            lines.append("(–ø—É—Å—Ç–æ)")
        lines.append("")
        lines.append("–î–æ–±–∞–≤–∏—Ç—å: /remember <—Ç–µ–∫—Å—Ç>")
        lines.append("–£–¥–∞–ª–∏—Ç—å: /forget <–Ω–æ–º–µ—Ä> –∏–ª–∏ /forget all")
        safe_edit_text(call.message.chat.id, call.message.message_id, "\n".join(lines), reply_markup=memory_keyboard())

    elif call.data == "memory_clear":
        with STATE_LOCK:
            s = get_settings(user_id)
            s["memory"] = []
            settings_store.save()
        refresh_system_prompt_in_history(user_id)
        safe_edit_text(call.message.chat.id, call.message.message_id, "üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞.", reply_markup=main_menu_keyboard(user_id))

    elif call.data.startswith("stop:"):
        try:
            job_id = int(call.data.split(":", 1)[1])
        except Exception:
            bot.answer_callback_query(call.id, "–ù–µ–≤–µ—Ä–Ω—ã–π job id.")
            return

        job = jobs.get(job_id)
        if not job:
            bot.answer_callback_query(call.id, "–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞/—É—Å—Ç–∞—Ä–µ–ª–∞.")
            return
        if job.user_id != user_id:
            bot.answer_callback_query(call.id, "–ù–µ–ª—å–∑—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á—É–∂—É—é –∑–∞–¥–∞—á—É.")
            return
        if job.done or job.canceled:
            bot.answer_callback_query(call.id, "–£–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
            return

        job.cancel_event.set()

        was_removed = False
        with SCHED_LOCK:
            if not job.started:
                q = user_queues.get(user_id)
                if q:
                    try:
                        q.remove(job_id)
                        job.canceled = True
                        was_removed = True
                    except ValueError:
                        pass
                SCHED_COND.notify_all()

        if was_removed:
            remove_user_message_by_job(user_id, job_id)
            safe_edit_text(job.chat_id, job.status_message_id, "üõë –û—Ç–º–µ–Ω–µ–Ω–æ (—É–¥–∞–ª–µ–Ω–æ –∏–∑ –æ—á–µ—Ä–µ–¥–∏).", reply_markup=None)
        else:
            safe_edit_text(job.chat_id, job.status_message_id, "üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é‚Ä¶", reply_markup=None)

        bot.answer_callback_query(call.id, "–û–∫.")


# =============================================================================
# MAIN MESSAGE HANDLER (ENQUEUE + reliability checks)
# =============================================================================

@bot.message_handler(content_types=["text", "photo"])
def handle_message(message: types.Message) -> None:
    global ACCEPTING_JOBS

    # –¥–µ–¥—É–ø –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∞–ø–¥–µ–π—Ç–æ–≤
    if is_duplicate_message(message.chat.id, message.message_id):
        return

    user_id = uid(message.from_user.id)

    # –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ–º –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –ø—Ä–∏ shutdown
    if SHUTDOWN_EVENT.is_set() or not ACCEPTING_JOBS:
        bot.send_message(message.chat.id, "–ë–æ—Ç —Å–µ–π—á–∞—Å –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è/–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")
        return

    # rate limit
    rl = check_rate_limit(user_id)
    if rl:
        bot.send_message(message.chat.id, rl, reply_markup=main_menu_keyboard(user_id))
        return

    # circuit breaker (LM Studio –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)
    if CB.is_open():
        bot.send_message(
            message.chat.id,
            "LLM –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–ø–µ—Ä–µ–≥—Ä—É–∑/–æ—à–∏–±–∫–∞). –ü–æ–ø—Ä–æ–±—É–π —á—É—Ç—å –ø–æ–∑–∂–µ.",
            reply_markup=main_menu_keyboard(user_id),
        )
        return

    with STATE_LOCK:
        if user_id not in chat_histories:
            init_history(user_id)
    refresh_system_prompt_in_history(user_id)

    job_id = next_job_id()
    status_msg = bot.reply_to(message, "‚è≥ –î–æ–±–∞–≤–ª—è—é –≤ –æ—á–µ—Ä–µ–¥—å‚Ä¶", reply_markup=stop_keyboard(job_id))

    has_img = message_has_image(message)
    try:
        if has_img and message.photo:
            file_id = message.photo[-1].file_id
            caption = build_photo_caption(message.caption)
            store_user_photo(user_id, file_id=file_id, caption=caption, job_id=job_id)
        else:
            text = (message.text or "").strip()
            if not text:
                safe_delete(message.chat.id, status_msg.message_id)
                bot.send_message(message.chat.id, "–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.", reply_markup=main_menu_keyboard(user_id))
                return
            store_user_text(user_id, text=text, job_id=job_id)
    except Exception as e:
        record_error(f"history write error: {type(e).__name__}: {e}")
        safe_delete(message.chat.id, status_msg.message_id)
        bot.send_message(message.chat.id, f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}", reply_markup=main_menu_keyboard(user_id))
        return

    # –û—Ü–µ–Ω–∫–∞ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    try:
        snap = snapshot_history_for_job(user_id, job_id)
        prompt_cost = token_estimator.estimate_messages(snap)
        used_cost = get_token_status(user_id).used
    except Exception as e:
        record_error(f"priority estimate failed: {type(e).__name__}: {e}")
        prompt_cost = TOKEN_LIMIT // 2
        used_cost = TOKEN_LIMIT // 2

    pr = compute_priority(
        user_id=user_id,
        prompt_tokens_estimate=prompt_cost,
        used_tokens_estimate=used_cost,
        has_image=has_img,
    )

    job = Job(
        job_id=job_id,
        user_id=user_id,
        chat_id=message.chat.id,
        status_message_id=status_msg.message_id,
        created_at=time.time(),
        priority=pr,
        has_image=has_img,
    )

    ok = enqueue_job(job)
    if not ok:
        remove_user_message_by_job(user_id, job_id)
        safe_delete(message.chat.id, status_msg.message_id)
        bot.send_message(
            message.chat.id,
            f"–û—á–µ—Ä–µ–¥—å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∞ (–ª–∏–º–∏—Ç {MAX_PENDING_PER_USER}). –ü–æ–¥–æ–∂–¥–∏ –∏–ª–∏ /stop —Ç–µ–∫—É—â—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.",
            reply_markup=main_menu_keyboard(user_id),
        )
        return

    qs = compute_queue_status_for_job(user_id, job_id)
    lines = [
        "üìå –ó–∞–ø—Ä–æ—Å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å.",
        f"üß∑ Job ID: {job_id}",
        f"‚≠ê –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {qs.priority}",
        f"üë§ –£ —Ç–µ–±—è –≤–ø–µ—Ä–µ–¥–∏ –∑–∞–¥–∞—á: {qs.user_ahead} (–∞–∫—Ç–∏–≤–Ω–∞—è: {'–¥–∞' if qs.user_has_active else '–Ω–µ—Ç'})",
        f"üî¢ –¢–≤–æ—è –ø–æ–∑–∏—Ü–∏—è —É —Ç–µ–±—è: {qs.user_position}",
        f"‚öôÔ∏è Active(global): {qs.global_active}/{MAX_ACTIVE_GLOBAL}",
        f"üì• Pending(global): {qs.global_pending}",
        f"ü§ñ LM Studio model: {resolve_lmstudio_model_id()}",
        "–ú–æ–∂–Ω–æ –æ—Ç–º–µ–Ω–∏—Ç—å –∫–Ω–æ–ø–∫–æ–π –Ω–∏–∂–µ.",
    ]

    safe_edit_text(message.chat.id, status_msg.message_id, "\n".join(lines), reply_markup=stop_keyboard(job_id))


# =============================================================================
# GRACEFUL SHUTDOWN
# =============================================================================

def graceful_shutdown(reason: str) -> None:
    global ACCEPTING_JOBS
    if SHUTDOWN_EVENT.is_set():
        return
    logger.info("Shutting down: %s", reason)
    ACCEPTING_JOBS = False
    SHUTDOWN_EVENT.set()

    # best-effort: cancel all pending/active jobs (don‚Äôt block forever)
    try:
        with SCHED_LOCK:
            for jid, j in list(jobs.items()):
                if j.done or j.canceled:
                    continue
                j.cancel_event.set()
    except Exception:
        pass

    try:
        bot.stop_polling()
    except Exception:
        pass

    # flush JSON
    try:
        settings_store.save()
    except Exception:
        pass
    try:
        history_store.save()
    except Exception:
        pass


def _sig_handler(signum: int, _frame: Any) -> None:
    graceful_shutdown(f"signal {signum}")


atexit.register(lambda: graceful_shutdown("atexit"))

try:
    signal.signal(signal.SIGINT, _sig_handler)
except Exception:
    pass
try:
    signal.signal(signal.SIGTERM, _sig_handler)
except Exception:
    pass


# =============================================================================
# BOOT
# =============================================================================

if __name__ == "__main__":
    logger.info(
        "BOT READY ‚úî owner=%s base_url=%s workers=%d max_active_global=%d skip_pending=%s",
        BOT_OWNER_ID,
        BASE_URL,
        WORKER_COUNT,
        MAX_ACTIVE_GLOBAL,
        SKIP_PENDING_UPDATES,
    )

    # Polling with safer defaults
    try:
        bot.polling(
            non_stop=True,
            skip_pending=SKIP_PENDING_UPDATES,
            timeout=POLLING_TIMEOUT,
            long_polling_timeout=LONG_POLLING_TIMEOUT,
        )
    except TypeError:
        # older telebot without long_polling_timeout
        bot.polling(
            non_stop=True,
            skip_pending=SKIP_PENDING_UPDATES,
            timeout=POLLING_TIMEOUT,
        )